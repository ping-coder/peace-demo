{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd135137-5154-438c-b93b-18b2f0315d99",
   "metadata": {},
   "source": [
    "# init and install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c108a-6ff6-4416-9629-508d25a10df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b1cc6-66ec-455d-9ee5-455081d978cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f26c135-8773-4fa1-bb4f-98ac062e1c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is the transcript and summary of the conversation.\n",
      "\n",
      "0:00:11 Speaker A: Okay, movies.\n",
      "0:00:13 Speaker A: What's your favorite movie?\n",
      "0:00:18 Speaker B: Uh probably Natural Born Killers.\n",
      "0:00:22 Speaker A: Natural Born Killers is amazing.\n",
      "0:00:25 Speaker A: I love I forgot his name.\n",
      "0:00:29 Speaker A: Woody Harrelson.\n",
      "0:00:30 Speaker B: Woody yeah, he's so cool.\n",
      "0:00:33 Speaker B: Um\n",
      "0:00:38 Speaker B: What movies do you not like?\n",
      "0:00:43 Speaker A: Not like?\n",
      "0:00:46 Speaker A: Uh okay, pretty much everything that Matt doesn't like.\n",
      "\n",
      "## Overall Summary\n",
      "Speaker A and Speaker B discuss their favorite movies, including Natural Born Killers, Talladega Nights, and Step Brothers. They also talk about horror films, with Speaker A expressing a preference for comedies. Speaker A mentions that they watched Lord of the Rings with **Matt**, who seems to have particular taste in movies. They plan to watch Return of the King that night. Speaker B recalls watching horror films with their dad and stepmom, and shares an anecdote about watching Texas Chainsaw Massacre at a friend's house with a creepy dad. \n",
      "\n",
      "## Core Events\n",
      "1. **Speaker A and Speaker B discuss their movie preferences.** Speaker A enjoys Natural Born Killers and Talladega Nights, while disliking Rush. Speaker B likes Natural Born Killers and Step Brothers. They both express a dislike for horror films, with Speaker A preferring comedies.\n",
      "2. **Speaker A reveals they watched Lord of the Rings with Matt.** Speaker A mentions watching Lord of the Rings with Matt, who preferred Rush. This suggests they are in a relationship or live together. They plan to watch Return of the King together that night.\n",
      "3. **Speaker B shares an anecdote about watching Texas Chainsaw Massacre at a friend's house.** Speaker B recalls watching the film at a friend's house in the country. The friend's dad was creepy and resembled the doctor from Back to the Future, making the experience unsettling.\n",
      "\n",
      "## Their Friends' characteristics\n",
      "1. **Matt**: Has a specific taste in movies, preferring action films like Rush.\n",
      "2. **Steph**: Watched Psycho with Speaker B and their brother on Halloween.\n",
      "3. **Franny**: Frequently went to the cinema with Speaker B and Steph.\n",
      "4. **Dad**: Enjoys horror films and introduced Speaker B to classics like Psycho and Texas Chainsaw Massacre.\n",
      "5. **Stepmom**: Showed the original Texas Chainsaw Massacre to Speaker B, Steph, and their half-cousins from Amsterdam on Halloween. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "\n",
    "project_id = \"peace-demo\"\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(model_name=\"gemini-1.5-pro-preview-0514\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "This is that two people are discussing their favorite movies. and identify the names of the speakers. \n",
    "If the name cannot be identified, use Speaker A and Speaker B instead. Please exclude the names of movie characters and their friends.\n",
    "Output first 10 sentences in the following format:\n",
    "<timestamp> <speaker's name>: <content>.\n",
    "\n",
    "After output the conversation, please summarize it with below format and the phone number, friend's name and time need to be in markdown bold format:\n",
    "## Overall Summary\n",
    "<summary and limit 100 words>\n",
    "\n",
    "## Core Events <Three core events are described in detailed language>\n",
    "1. <event>\n",
    "2. <event>\n",
    "3. <event>\n",
    "\n",
    "## Their Friends' characteristics <Five Friends>\n",
    "1. <name>: <Describe the characteristics>\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "audio_file_uri = \"gs://peace-demo-temp-us-central1/dialog_en_1hr.wav\"\n",
    "audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
    "\n",
    "# Generation Config\n",
    "config = GenerationConfig(\n",
    "    max_output_tokens=8192, temperature=0.6, top_p=0.5, top_k=24\n",
    ")\n",
    "\n",
    "contents = [audio_file, prompt]\n",
    "\n",
    "response = model.generate_content(contents, generation_config=config)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb623a-1877-4d93-9156-78185256e2d5",
   "metadata": {},
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95c2e84c-816b-4aec-b23b-bbfab88c84f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Different Reply Styles to the Customer Complaint:\n",
      "\n",
      "**Formal Response:**\n",
      "\n",
      "\"I understand your frustration, and I apologize for the inconvenience this delay is causing. I'll make sure to escalate this issue to the appropriate team immediately. They will have the necessary expertise to assist you further and ensure a swift resolution.\"\n",
      "\n",
      "\n",
      "**Casual Response:** \n",
      "\n",
      "\"No worries, I totally get how annoying that is!  It's definitely not cool for Amazon to be holding onto your money like that. Hang tight, I'm grabbing someone else from the team who can hopefully sort this out for you ASAP!\"\n",
      "\n",
      "\n",
      "**Humorous Response:** \n",
      "\n",
      "\"Oh boy, battling Amazon's virtual assistant army to reach a human? I feel you! It's like trying to find a unicorn riding a dragon. But fear not,  I'll summon my inner superhero and get you a real person who can actually help. Hold on to your hats!\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "\n",
    "project_id = \"peace-demo\"\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(model_name=\"gemini-1.5-pro-preview-0514\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "\"Based on the dialogue in the screenshot, generate three different styles of replies using the same language as the conversation. Provide:\n",
    "\n",
    "A formal response\n",
    "A casual response\n",
    "A humorous response\n",
    "\n",
    "Maintain the tone and context of the original dialogue while adjusting the style for each reply.\"\n",
    "\n",
    "Explanation:\n",
    "Formal Response: This style uses polite and professional language, often suitable for official or business contexts.\n",
    "Casual Response: This is more relaxed and conversational, appropriate for friends or informal settings.\n",
    "Humorous Response: This style incorporates light-hearted or witty elements to make the reply entertaining.\n",
    "\"\"\"\n",
    "\n",
    "file_uri = \"gs://peace-demo-temp-us-central1/chat_sceenshot/Picture1.png\"\n",
    "file = Part.from_uri(file_uri, mime_type=\"image/png\")\n",
    "    \n",
    "contents = [file, prompt]\n",
    "\n",
    "\n",
    "# Generation Config\n",
    "config = GenerationConfig(\n",
    "    max_output_tokens=8192, temperature=0.6, top_p=0.5, top_k=24,\n",
    "    response_mime_type=\"application/text\"\n",
    ")\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ece58d-68c7-4a34-96f9-015a0c34af1e",
   "metadata": {},
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c34cdbb-99d7-4653-a473-2c4273070c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://peace-demo-temp-us-central1/rag_synthesis_samples.txt...\n",
      "- [1 files][ 17.8 KiB/ 17.8 KiB]                                                \n",
      "Operation completed over 1 objects/17.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp \"gs://peace-demo-temp-us-central1/rag_synthesis_samples.txt\" ./rag_synthesis_samples.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbe4d416-6e66-45ef-8970-9ad9fc82eb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EXAMPLE_1>\n",
      "<Question>\n",
      "my screen is too dark\n",
      "</Question>\n",
      "\"Keywords\": ['screen brightness', 'dark mode', 'bedtime mode', 'grayscale']\n",
      "\"Answer\": To adjust your screen's brightness, you can access the **screen brightness** settings. If your device is in **dark mode**, try disabling it for a brighter display. Additionally, check if **bedtime mode** is activated, as it often reduces brightness. **Grayscale** mode can also make the screen appear darker.\n",
      "\"Referring\": [0, 1, 4, 5, 9]\n",
      "<EXAMPLE_2>\n",
      "<Question>\n",
      "find some file about Retrieval-Augmented Generation\n",
      "\n",
      "\n",
      "</Question>\n",
      "\"Keywords\": ['Retrieval-Augmented Generation', 'EvaluatingtheEffectivenessofRetrieval-Augmented.pdf', 'BenchmarkingLargeLanguageModelsinRetrieval-AugmentedGeneration.pdf']\n",
      "\"Answer\": I found two files related to **Retrieval-Augmented Generation**:  \"**EvaluatingtheEffectivenessofRetrieval-Augmented.pdf**\" and \"**BenchmarkingLargeLanguageModelsinRetrieval-AugmentedGeneration.pdf**\".\n",
      "\"Referring\": [0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "\n",
    "project_id = \"peace-demo\"\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(model_name=\"gemini-1.5-pro-preview-0514\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Multiple Outputs for each example:\n",
    "\"Keywords\":Keywords of the answer\n",
    "\"Answer\":synthesized answer with keywords bold with markdown\n",
    "\"Referring\":list referring the chunk number\n",
    "\"\"\"\n",
    "\n",
    "txt_file_uri = \"./rag_synthesis_samples.txt\"\n",
    "with open(txt_file_uri, 'r') as file:\n",
    "    chuncks = file.read()  # Read the entire file content\n",
    "    \n",
    "contents = [chuncks, prompt]\n",
    "\n",
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"Keywords\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"Answer\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"Referring\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Generation Config\n",
    "config = GenerationConfig(\n",
    "    max_output_tokens=8192, temperature=0.6, top_p=0.5, top_k=24,\n",
    "    response_mime_type=\"application/json\", response_schema=response_schema\n",
    ")\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b96e6-0e92-4394-af62-78f8541f0a64",
   "metadata": {},
   "source": [
    "# question 4,5,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228a9cbb-7a43-4375-965a-1718e6d92b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Image editing failed with the following error: Project or the user is not allowlisted to turn off child detection filter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Image editing failed with the following error: Project or the user is not allowlisted to turn off child detection filter\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.194.95:443 {grpc_message:\"Image editing failed with the following error: Project or the user is not allowlisted to turn off child detection filter\", grpc_status:9, created_time:\"2024-09-13T17:56:28.068472094+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#prompt=\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Maintain the wall's color and texture exactly as they are. Ensure that the area where the clock was removed blends seamlessly with the rest of the wall, preserving its original appearance.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mIgnore the people in the picture and don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt make any changes\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minpainting-remove\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional parameters\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morange fence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Describes the object being removed (i.e., \"person\"),\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave(location\u001b[38;5;241m=\u001b[39moutput_file, include_generation_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/vision_models/_vision_models.py:769\u001b[0m, in \u001b[0;36mImageGenerationModel.edit_image\u001b[0;34m(self, prompt, base_image, mask, negative_prompt, number_of_images, guidance_scale, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, seed, output_gcs_uri, safety_filter_level, person_generation)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21medit_image\u001b[39m(\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Edits an existing image based on text prompt.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m        An `ImageGenerationResponse` object.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medit_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegmentation_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduct_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduct_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mime_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_quality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_quality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_watermark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not supported for editing yet\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_filter_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_filter_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperson_generation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/vision_models/_vision_models.py:581\u001b[0m, in \u001b[0;36mImageGenerationModel._generate_images\u001b[0;34m(self, prompt, negative_prompt, number_of_images, width, height, aspect_ratio, guidance_scale, seed, base_image, mask, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, output_gcs_uri, add_watermark, safety_filter_level, person_generation)\u001b[0m\n\u001b[1;32m    578\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m person_generation\n\u001b[1;32m    579\u001b[0m     shared_generation_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m person_generation\n\u001b[0;32m--> 581\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m generated_images: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneratedImage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(response\u001b[38;5;241m.\u001b[39mpredictions):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:2105\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   2091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   2092\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2093\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mjson_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2102\u001b[0m         ),\n\u001b[1;32m   2103\u001b[0m     )\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2105\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   2112\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:851\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 851\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Image editing failed with the following error: Project or the user is not allowlisted to turn off child detection filter"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.vision_models import Image, ImageGenerationModel\n",
    "\n",
    "project_id = \"peace-demo\"\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = ImageGenerationModel.from_pretrained(\"imagegeneration@006\")\n",
    "#model = ImageGenerationModel.from_pretrained(\"imagen-3.0-generate-001\")\n",
    "base_img = Image.load_from_file(location=\"gs://peace-demo-temp-us-central1/cmp_samples/3.jpg\")\n",
    "mask_img = Image.load_from_file(location=\"gs://peace-demo-temp-us-central1/cmp_samples/3_mask.jpg\")\n",
    "output_file = \"./cmp_samples_output_3.jpg\"\n",
    "\n",
    "#prompt=\"\"\"\n",
    "#Maintain the wall's color and texture exactly as they are. Ensure that the area where the clock was removed blends seamlessly with the rest of the wall, preserving its original appearance.\n",
    "#\"\"\"\n",
    "\n",
    "prompt=\"\"\"\n",
    "Ignore the people in the picture and don't make any changes\n",
    "\"\"\"\n",
    "\n",
    "images = model.edit_image(\n",
    "    base_image=base_img,\n",
    "    mask=mask_img,\n",
    "    prompt=prompt,\n",
    "    edit_mode=\"inpainting-remove\",\n",
    "    guidance_scale=21,\n",
    "    seed=1,\n",
    "    person_generation=\"allow_all\",\n",
    "    # Optional parameters\n",
    "    negative_prompt=\"orange fence\", # Describes the object being removed (i.e., \"person\"),\n",
    "    number_of_images=1,\n",
    ")\n",
    "\n",
    "images[0].save(location=output_file, include_generation_parameters=False)\n",
    "\n",
    "images[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cc00003-c2aa-4a84-926e-7bfe995421eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./cmp_samples_output_1.jpg [Content-Type=image/jpeg]...\n",
      "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
      "Operation completed over 1 objects/3.0 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp \"./cmp_samples_output_2.jpg\" \"gs://peace-demo-temp-us-central1/cmp_samples_output/2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e9f78-1533-45c3-ae05-c6d408ad0356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
